{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyDvil_3p0NA"
      },
      "source": [
        "#  Creating Models with TensorFlow and PyTorch\n",
        "\n",
        "- create an entirely new model with an architecture you define yourself. \n",
        "\n",
        "- [DeepChem](https://github.com/deepchem/deepchem/tree/master/examples/tutorials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6KbbaYENp0NE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395a448a-c249-42e8-9e1c-d53f90936ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.6.1-py3-none-any.whl (608 kB)\n",
            "\u001b[K     |████████████████████████████████| 608 kB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.0.2)\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.8 MB 31 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi->deepchem) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Installing collected packages: rdkit-pypi, deepchem\n",
            "Successfully installed deepchem-2.6.1 rdkit-pypi-2022.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre deepchem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUG4mG9rp0NH"
      },
      "source": [
        "- two different approaches:\n",
        " - use TensorFlow/PyTorch APIs or DeepChem APIs \n",
        "- For the former case, DeepChem's `Dataset` class has methods for easily adapting it to use with other frameworks.  \n",
        " - `make_tf_dataset()` returns a `tensorflow.data.Dataset` object that iterates over the data.  `make_pytorch_dataset()` returns a `torch.utils.data.IterableDataset` that iterates over the data.  \n",
        " - This lets you use DeepChem's datasets, loaders, featurizers, transformers, splitters, etc. and easily integrate them into your existing TensorFlow or PyTorch code.\n",
        "\n",
        "- DeepChem also provides many other useful features.  The other approach, which lets you use those features, is to wrap your model in a DeepChem `Model` object.  Let's look at how to do that.\n",
        "\n",
        "## KerasModel\n",
        "\n",
        "`KerasModel` is a subclass of DeepChem's `Model` class.  It acts as a wrapper around a `tensorflow.keras.Model`.  Let's see an example of using it.  For this example, we create a simple sequential model consisting of two dense layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N4slcJjQp0NI"
      },
      "outputs": [],
      "source": [
        "import deepchem as dc\n",
        "import tensorflow as tf\n",
        "\n",
        "keras_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dropout(rate=0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model = dc.models.KerasModel(keras_model, dc.models.losses.L2Loss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HJiUFj_p0NK"
      },
      "source": [
        "- specify the loss function to use when training the model, in this case L<sub>2</sub> loss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dlCT4pk4p0NL",
        "outputId": "19f3e51e-4fe2-43ee-ec39-0f39afcbed72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'pearson_r2_score': 0.9765891230353816}\n",
            "test set score: {'pearson_r2_score': 0.7516208742255748}\n"
          ]
        }
      ],
      "source": [
        "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='ECFP', splitter='random')\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "model.fit(train_dataset, nb_epoch=50)\n",
        "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0-wyR0Rp0NM"
      },
      "source": [
        "## TorchModel\n",
        "\n",
        "`TorchModel` works just like `KerasModel`, except it wraps a `torch.nn.Module`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "14aM8LLwp0NN",
        "outputId": "a9a32de1-a86b-44c3-bdfc-1601b7387906",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'pearson_r2_score': 0.9774231332404432}\n",
            "test set score: {'pearson_r2_score': 0.7606061522867582}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "pytorch_model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1024, 1000),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5),\n",
        "    torch.nn.Linear(1000, 1)\n",
        ")\n",
        "model = dc.models.TorchModel(pytorch_model, dc.models.losses.L2Loss())\n",
        "\n",
        "model.fit(train_dataset, nb_epoch=50)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIuncKuIp0NP"
      },
      "source": [
        "## Computing Losses\n",
        "\n",
        "Now let's see a more advanced example.  In the above models, the loss was computed directly from the model's output.  Often that is fine, but not always.  Consider a classification model that outputs a probability distribution.  While it is possible to compute the loss from the probabilities, it is more numerically stable to compute it from the logits.\n",
        "\n",
        "To do this, we create a model that returns multiple outputs, both probabilities and logits.  `KerasModel` and `TorchModel` let you specify a list of \"output types\".  If a particular output has type `'prediction'`, that means it is a normal output that should be returned when you call `predict()`.  If it has type `'loss'`, that means it should be passed to the loss function in place of the normal outputs.\n",
        "\n",
        "Sequential models do not allow multiple outputs, so instead we use a subclassing style model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-2g9yxs3p0NR"
      },
      "outputs": [],
      "source": [
        "class ClassificationModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(1000, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        y = self.dense1(inputs)\n",
        "        if training:\n",
        "            y = tf.nn.dropout(y, 0.5)\n",
        "        logits = self.dense2(y)\n",
        "        output = tf.nn.sigmoid(logits)\n",
        "        return output, logits\n",
        "\n",
        "keras_model = ClassificationModel()\n",
        "output_types = ['prediction', 'loss']\n",
        "model = dc.models.KerasModel(keras_model, dc.models.losses.SigmoidCrossEntropy(), output_types=output_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SD72U4Zp0NS"
      },
      "source": [
        "We can train our model on the BACE dataset.  This is a binary classification task that tries to predict whether a molecule will inhibit the enzyme BACE-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VwxiVh4cp0NT",
        "outputId": "887e055d-c57d-4800-e51c-4cfba8372c4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'roc_auc_score': 0.9996228260110358}\n",
            "test set score: {'roc_auc_score': 0.7702898550724637}\n"
          ]
        }
      ],
      "source": [
        "tasks, datasets, transformers = dc.molnet.load_bace_classification(feturizer='ECFP', splitter='scaffold')\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "model.fit(train_dataset, nb_epoch=100)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we will create a custom Classifier Model class to be used with `TorchModel`. Using similar reasoning to the above `KerasModel`, a custom model allows for easy capturing of the unscaled output (logits in Tensorflow) of the second dense layer. The custom class allows definition of how forward pass is done; enabling capture of the logits right before the final sigmoid is applied to produce the prediction. \n",
        "\n",
        "Finally, an instance of `ClassificationModel` is coupled with a loss function that requires both the prediction and logits to produce an instance of `TorchModel` to train. "
      ],
      "metadata": {
        "id": "sf4tIfaPs1Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationModel(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.dense1 = torch.nn.Linear(1024, 1000)\n",
        "        self.dense2 = torch.nn.Linear(1000, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        y = torch.nn.functional.relu( self.dense1(inputs) )\n",
        "        y = torch.nn.functional.dropout(y, p=0.5, training=self.training)\n",
        "        logits = self.dense2(y)\n",
        "        output = torch.sigmoid(logits)\n",
        "        return output, logits\n",
        "\n",
        "torch_model = ClassificationModel()\n",
        "output_types = ['prediction', 'loss']\n",
        "model = dc.models.TorchModel(torch_model, dc.models.losses.SigmoidCrossEntropy(), output_types=output_types)"
      ],
      "metadata": {
        "id": "jMOOa20Yszz2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the same BACE dataset. As before, the model will try to do a binary classification task that tries to predict whether a molecule will inhibit the enzyme BACE-1."
      ],
      "metadata": {
        "id": "LQqp2m6iGdgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks, datasets, transformers = dc.molnet.load_bace_classification(feturizer='ECFP', splitter='scaffold')\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "model.fit(train_dataset, nb_epoch=100)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ],
      "metadata": {
        "id": "N49-Xsic-9iY",
        "outputId": "6647ad6b-035f-407f-a37c-ac87870c869d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'roc_auc_score': 0.9996367954180345}\n",
            "test set score: {'roc_auc_score': 0.7649003623188406}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LY7QO9ip0NU"
      },
      "source": [
        "## Other Features\n",
        "\n",
        "`KerasModel` and `TorchModel` have lots of other features.  Here are some of the more important ones.\n",
        "\n",
        "- Automatically saving checkpoints during training.\n",
        "- Logging progress to the console, to [TensorBoard](https://www.tensorflow.org/tensorboard), or to [Weights & Biases](https://docs.wandb.com/).\n",
        "- Custom loss functions that you define with a function of the form `f(outputs, labels, weights)`.\n",
        "- Early stopping using the `ValidationCallback` class.\n",
        "- Loading parameters from pre-trained models.\n",
        "- Estimating uncertainty in model outputs.\n",
        "- Identifying important features through saliency mapping.\n",
        "\n",
        "By wrapping your own models in a `KerasModel` or `TorchModel`, you get immediate access to all these features.  See the API documentation for full details on them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KNAp_wD0Vv3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "c_81_DC_5_Creating_Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}